\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{vaswani2017attention}
\citation{radford2018improving,radford2019language,brown2020language}
\citation{dosovitskiy2021an,liu2021swin}
\citation{liu2023visual,wang2024git}
\citation{ying2021graphormer}
\citation{wang2023dsvt,wang2023unitr}
\citation{vaswani2017attention}
\citation{liu2023visual,zhu2023minigpt,wang2023image}
\citation{dunford1988linear}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{biderman2023pythia,kaplan2020scaling}
\citation{vaswani2017attention}
\citation{radford2018improving,touvron2023llama}
\citation{dosovitskiy2021an}
\citation{liu2023visual,wang2024git,wang2023unitr,wang2022ofa}
\citation{chen2021decision}
\citation{yun2019graph}
\citation{radford2018improving,radford2019language,brown2020language}
\citation{kaplan2020scaling}
\citation{biderman2023pythia}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Traditionally, large transformer architectures are trained from scratch without reusing previous smaller-scale models (represented by blue dots on the left). In this paper, we propose a novel fully attention-based architecture that allows scaling model incrementally, thus greatly reducing the overall cost of training large transformer architectures (depicted by red dots on the left). The right panel delineates a comparison between conventional Transformer and our Tokenformer.}}{2}{figure.1}\protected@file@percent }
\newlabel{fig:intro_figure}{{1}{2}{Traditionally, large transformer architectures are trained from scratch without reusing previous smaller-scale models (represented by blue dots on the left). In this paper, we propose a novel fully attention-based architecture that allows scaling model incrementally, thus greatly reducing the overall cost of training large transformer architectures (depicted by red dots on the left). The right panel delineates a comparison between conventional Transformer and our Tokenformer}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}\protected@file@percent }
\citation{chen2015net2net,chen2021bert2bert}
\citation{gong2019efficient}
\citation{wang2023learning}
\citation{vaswani2017attention}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Preliminaries}{3}{subsection.3.1}\protected@file@percent }
\newlabel{sec:preliminary}{{3.1}{3}{Preliminaries}{subsection.3.1}{}}
\newlabel{eq:qkv}{{1}{3}{Preliminaries}{equation.3.1}{}}
\newlabel{eq:token_att}{{2}{3}{Preliminaries}{equation.3.2}{}}
\newlabel{eq:output}{{3}{3}{Preliminaries}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Tokenformer\xspace  }{3}{subsection.3.2}\protected@file@percent }
\newlabel{sec:tokenformer}{{3.2}{3}{\ourmethod }{subsection.3.2}{}}
\citation{hendrycks2016gaussian}
\citation{ba2016layer,zhang2019root}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces {Tokenformer\xspace  } is a fully attention-driven architecture featuring a new token-\textbf  {P}arameter \textbf  {attention} (Pattention) layer. The Pattention uses a set of learnable tokens to represent model parameters and lets the input tokens attend to them. As the model scales, Tokenformer\xspace  adds new learnable tokens to expand the existing key-value parameter sets, while keeping the feature dimension constant and leaving the rest of the computation unaffected. }}{4}{figure.2}\protected@file@percent }
\newlabel{fig:att_token_param}{{2}{4}{{\ourmethod } is a fully attention-driven architecture featuring a new token-\textbf {P}arameter \textbf {attention} (Pattention) layer. The Pattention uses a set of learnable tokens to represent model parameters and lets the input tokens attend to them. As the model scales, \ourmethod adds new learnable tokens to expand the existing key-value parameter sets, while keeping the feature dimension constant and leaving the rest of the computation unaffected}{figure.2}{}}
\newlabel{eq:token-parameter-1}{{4}{4}{\ourmethod }{equation.3.4}{}}
\newlabel{eq:token-parameter-2}{{5}{4}{\ourmethod }{equation.3.5}{}}
\citation{radford2018improving}
\citation{hu2022lora}
\newlabel{eq:our_qkv}{{8}{5}{\ourmethod }{equation.3.8}{}}
\newlabel{eq:our_token_att}{{9}{5}{\ourmethod }{equation.3.9}{}}
\newlabel{eq:our_output}{{10}{5}{\ourmethod }{equation.3.10}{}}
\newlabel{eq:our_ffn}{{11}{5}{\ourmethod }{equation.3.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Progressive Model Scaling}{5}{subsection.3.3}\protected@file@percent }
\newlabel{sec:whytokenformer}{{3.3}{5}{Progressive Model Scaling}{subsection.3.3}{}}
\newlabel{eq:token-parameter-3}{{13}{5}{Progressive Model Scaling}{equation.3.13}{}}
\citation{Gokaslan2019OpenWeb}
\citation{radford2019language}
\citation{nanogpt,kaplan2020scaling}
\citation{loshchilov2018decoupled}
\citation{nanogpt}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Evaluating model scaling costs through cumulative computational budgets. The Transformer baseline incurs expenses for each individual scaling step performed independently from scratch, whereas Tokenformer\xspace  aggregates costs across all scaling stages, including training a 124M model initially, progressively scaling to 354M, 757M, and 1.4B parameters.}}{6}{figure.3}\protected@file@percent }
\newlabel{fig:scaling_accum}{{3}{6}{Evaluating model scaling costs through cumulative computational budgets. The Transformer baseline incurs expenses for each individual scaling step performed independently from scratch, whereas \ourmethod aggregates costs across all scaling stages, including training a 124M model initially, progressively scaling to 354M, 757M, and 1.4B parameters}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Evaluating model scaling costs by measuring the budget required at each scaling stage. The Transformer baselines used are consistent with those depicted in Figure\nobreakspace  {}\ref  {fig:scaling_accum}, trained with 30B and 300B tokens. Similarly, for Tokenformer\xspace  , the cost is the budget required for each incremental scaling step from a smaller one. All the experiments were conducted on TPU v4 hardware.}}{6}{figure.4}\protected@file@percent }
\newlabel{fig:scaling_individual}{{4}{6}{Evaluating model scaling costs by measuring the budget required at each scaling stage. The Transformer baselines used are consistent with those depicted in Figure~\ref {fig:scaling_accum}, trained with 30B and 300B tokens. Similarly, for \ourmethod , the cost is the budget required for each incremental scaling step from a smaller one. All the experiments were conducted on TPU v4 hardware}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}\protected@file@percent }
\newlabel{sec:xp}{{4}{6}{Experiments}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Progressive Model Scaling}{6}{subsection.4.1}\protected@file@percent }
\newlabel{sec:progressive_scaling}{{4.1}{6}{Progressive Model Scaling}{subsection.4.1}{}}
\citation{biderman2023pythia}
\citation{biderman2023pythia}
\citation{biderman2023pythia}
\citation{black2021gpt}
\citation{zhang2022opt}
\citation{biderman2023pythia}
\citation{black2021gpt}
\citation{zhang2022opt}
\citation{biderman2023pythia}
\citation{biderman2023pythia}
\citation{biderman2023pythia}
\citation{dosovitskiy2021an}
\citation{touvron2021training}
\citation{he2022masked}
\citation{dosovitskiy2021an}
\citation{he2022masked}
\citation{he2022masked}
\citation{he2022masked}
\citation{gao2020pile}
\citation{biderman2023pythia}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces (\textbf  {Zero-shot Evaluations.}) The best performance for each model size is highlighted in bold. Our comparisons are made with publicly available transformer-based LMs with various tokenizers. Following Pythia\nobreakspace  {}\citep  {biderman2023pythia}, our model is trained for up to 300B tokens on pile dataset. }}{7}{table.1}\protected@file@percent }
\newlabel{tab:llm_benchmark}{{1}{7}{(\textbf {Zero-shot Evaluations.}) The best performance for each model size is highlighted in bold. Our comparisons are made with publicly available transformer-based LMs with various tokenizers. Following Pythia~\citep {biderman2023pythia}, our model is trained for up to 300B tokens on pile dataset}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces (\textbf  {Image Classification.}) Comparison of standard vision transformer on ImageNet-1K. The training hyperparameters are completely consistent\nobreakspace  {}(batch size, learning rate, etc.) with \citet  {he2022masked}. $\dag  $ denotes models where the parameter size has been matched to that of the standard ViT.}}{7}{table.2}\protected@file@percent }
\newlabel{tab:visual_benchmark}{{2}{7}{(\textbf {Image Classification.}) Comparison of standard vision transformer on ImageNet-1K. The training hyperparameters are completely consistent~(batch size, learning rate, etc.) with \citet {he2022masked}. $\dag $ denotes models where the parameter size has been matched to that of the standard ViT}{table.2}{}}
\citation{biderman2023pythia}
\citation{dosovitskiy2021an}
\citation{deng2009imagenet}
\citation{MMDetection}
\citation{he2022masked}
\citation{chen2015net2net}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Parameter counts and training compute estimates for Transformer and our Tokenformer\xspace  . Sub-leading terms such as nonlinearities, biases, and layer normalization are omitted.}}{8}{table.3}\protected@file@percent }
\newlabel{tab:flops_and_param}{{3}{8}{Parameter counts and training compute estimates for Transformer and our \ourmethod . Sub-leading terms such as nonlinearities, biases, and layer normalization are omitted}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The relationship between FLOPs and text length for both Transformer and Tokenformer\xspace  . As shown in Table\nobreakspace  {}\ref  {tab:flops_and_param}, Transformer exhibits an increase in computational cost for token-token interactions as $d_{\text  {model}}$ scales upwards. Our Tokenformer\xspace  model, however, offers a flexible parameter scaling mechanism that maintains $d_{\text  {token}}$ at a constant value. This strategy results in controllable computational costs for token-token interactions and markedly enhances the efficiency of long-text modeling.}}{8}{figure.5}\protected@file@percent }
\newlabel{fig:flops_figure}{{5}{8}{The relationship between FLOPs and text length for both Transformer and \ourmethod . As shown in Table~\ref {tab:flops_and_param}, Transformer exhibits an increase in computational cost for token-token interactions as $d_{\text {model}}$ scales upwards. Our \ourmethod model, however, offers a flexible parameter scaling mechanism that maintains $d_{\text {token}}$ at a constant value. This strategy results in controllable computational costs for token-token interactions and markedly enhances the efficiency of long-text modeling}{figure.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Benchmarking of Model Expressiveness}{8}{subsection.4.2}\protected@file@percent }
\newlabel{sec:llm}{{4.2}{8}{Benchmarking of Model Expressiveness}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison with standard transformer}{8}{subsection.4.3}\protected@file@percent }
\newlabel{sec:compare_with_transformer}{{4.3}{8}{Comparison with standard transformer}{subsection.4.3}{}}
\citation{wei2022chain}
\citation{effcienttransformer}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Loss curves comparing pre-trained Transformer and Tokenformer\xspace  as their parameters are scaled during continued training on enwik8.}}{9}{figure.6}\protected@file@percent }
\newlabel{fig:zero_init}{{6}{9}{Loss curves comparing pre-trained Transformer and \ourmethod as their parameters are scaled during continued training on enwik8}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Performance benchmarking on incremental model scaling between Transformer with Net2Net scheme and our Tokenformer\xspace  .}}{9}{figure.7}\protected@file@percent }
\newlabel{fig:comparision_tf_to}{{7}{9}{Performance benchmarking on incremental model scaling between Transformer with Net2Net scheme and our \ourmethod }{figure.7}{}}
\citation{hendrycks2016gaussian}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Ablation Study}{10}{subsection.4.4}\protected@file@percent }
\newlabel{sec:ablation}{{4.4}{10}{Ablation Study}{subsection.4.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Ablation of Softmax part on ImageNet classification with base model.}}{10}{table.4}\protected@file@percent }
\newlabel{tab:softmax}{{4}{10}{Ablation of Softmax part on ImageNet classification with base model}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Ablation of non-parametric layer normalization on ImageNet classification with base model.}}{10}{table.5}\protected@file@percent }
\newlabel{tab:layernorm-nonparam}{{5}{10}{Ablation of non-parametric layer normalization on ImageNet classification with base model}{table.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Future Work}{10}{section.5}\protected@file@percent }
\bibdata{arxiv}
\bibcite{radford2019language}{{1}{2019}{{Alec et~al.}}{{Alec, Jeffrey, Rewon, David, Dario, Ilya, et~al.}}}
\bibcite{ba2016layer}{{2}{2016}{{Ba}}{{}}}
\bibcite{biderman2023pythia}{{3}{2023}{{Biderman et~al.}}{{Biderman, Schoelkopf, Anthony, Bradley, Oâ€™Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.}}}
\bibcite{black2021gpt}{{4}{2021}{{Black et~al.}}{{Black, Gao, Wang, Leahy, and Biderman}}}
\bibcite{black2022gpt}{{5}{2022}{{Black et~al.}}{{Black, Biderman, Hallahan, Anthony, Gao, Golding, He, Leahy, McDonell, Phang, et~al.}}}
\bibcite{brown2020language}{{6}{2020}{{Brown et~al.}}{{Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.}}}
\bibcite{chen2021bert2bert}{{7}{2021{a}}{{Chen et~al.}}{{Chen, Yin, Shang, Jiang, Qin, Wang, Wang, Chen, Liu, and Liu}}}
\bibcite{chen2021decision}{{8}{2021{b}}{{Chen et~al.}}{{Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch}}}
\bibcite{chen2015net2net}{{9}{2015}{{Chen et~al.}}{{Chen, Goodfellow, and Shlens}}}
\bibcite{deng2009imagenet}{{10}{2009}{{Deng et~al.}}{{Deng, Dong, Socher, Li, Li, and Fei-Fei}}}
\bibcite{dosovitskiy2021an}{{11}{2021}{{Dosovitskiy et~al.}}{{Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and Houlsby}}}
\bibcite{dunford1988linear}{{12}{1988}{{Dunford \& Schwartz}}{{Dunford and Schwartz}}}
\bibcite{fu2023hungry}{{13}{2023}{{Fu et~al.}}{{Fu, Dao, Saab, Thomas, Rudra, and Re}}}
\bibcite{gao2020pile}{{14}{2020}{{Gao et~al.}}{{Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.}}}
\bibcite{Gokaslan2019OpenWeb}{{15}{2019}{{Gokaslan \& Cohen}}{{Gokaslan and Cohen}}}
\bibcite{gong2019efficient}{{16}{2019}{{Gong et~al.}}{{Gong, He, Li, Qin, Wang, and Liu}}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{11}{section.6}\protected@file@percent }
\bibcite{mamba}{{17}{2023}{{Gu \& Dao}}{{Gu and Dao}}}
\bibcite{he2022masked}{{18}{2022}{{He et~al.}}{{He, Chen, Xie, Li, Doll{\'a}r, and Girshick}}}
\bibcite{hendrycks2016gaussian}{{19}{2016}{{Hendrycks \& Gimpel}}{{Hendrycks and Gimpel}}}
\bibcite{hu2022lora}{{20}{2022}{{Hu et~al.}}{{Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}}}
\bibcite{kaplan2020scaling}{{21}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}}}
\bibcite{nanogpt}{{22}{2022}{{Karpathy}}{{}}}
\bibcite{liu2023visual}{{23}{2023}{{Liu et~al.}}{{Liu, Li, Wu, and Lee}}}
\bibcite{liu2021swin}{{24}{2021}{{Liu et~al.}}{{Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and Guo}}}
\bibcite{loshchilov2017decoupled}{{25}{2019}{{Loshchilov}}{{}}}
\bibcite{loshchilov2018decoupled}{{26}{2019}{{Loshchilov \& Hutter}}{{Loshchilov and Hutter}}}
\bibcite{mahoney2011large}{{27}{2011}{{Mahoney}}{{}}}
\bibcite{MMDetection}{{28}{2018}{{MMDetection Contributors}}{{}}}
\bibcite{peng2023rwkv}{{29}{2023}{{Peng et~al.}}{{Peng, Alcaide, Anthony, Albalak, Arcadinho, Biderman, Cao, Cheng, Chung, Derczynski, Du, Grella, GV, He, Hou, Kazienko, Kocon, Kong, Koptyra, Lau, Lin, Mantri, Mom, Saito, Song, Tang, Wind, Wo{\'z}niak, Zhang, Zhou, Zhu, and Zhu}}}
\bibcite{radford2018improving}{{30}{2018}{{Radford et~al.}}{{Radford, Narasimhan, Salimans, Sutskever, et~al.}}}
\bibcite{effcienttransformer}{{31}{2020}{{Tay et~al.}}{{Tay, Dehghani, Bahri, and Metzler}}}
\bibcite{touvron2021training}{{32}{2021}{{Touvron et~al.}}{{Touvron, Cord, Douze, Massa, Sablayrolles, and J{\'e}gou}}}
\bibcite{touvron2023llama}{{33}{2023}{{Touvron et~al.}}{{Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.}}}
\bibcite{vaswani2017attention}{{34}{2017}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{wang2023dsvt}{{35}{2023{a}}{{Wang et~al.}}{{Wang, Shi, Shi, Lei, Wang, He, Schiele, and Wang}}}
\bibcite{wang2023unitr}{{36}{2023{b}}{{Wang et~al.}}{{Wang, Tang, Shi, Li, Li, Schiele, and Wang}}}
\bibcite{wang2024git}{{37}{2024}{{Wang et~al.}}{{Wang, Tang, Jiang, Shi, Naeem, Li, Schiele, and Wang}}}
\bibcite{wang2023learning}{{38}{2023{c}}{{Wang et~al.}}{{Wang, Panda, Hennigen, Greengard, Karlinsky, Feris, Cox, Wang, and Kim}}}
\bibcite{wang2022ofa}{{39}{2022}{{Wang et~al.}}{{Wang, Yang, Men, Lin, Bai, Li, Ma, Zhou, Zhou, and Yang}}}
\bibcite{wang2023image}{{40}{2023{d}}{{Wang et~al.}}{{Wang, Bao, Dong, Bjorck, Peng, Liu, Aggarwal, Mohammed, Singhal, Som, et~al.}}}
\bibcite{wei2022chain}{{41}{2022}{{Wei et~al.}}{{Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.}}}
\bibcite{ying2021graphormer}{{42}{2021}{{Ying et~al.}}{{Ying, Cai, Luo, Zheng, Ke, He, Shen, and Liu}}}
\bibcite{yun2019graph}{{43}{2019}{{Yun et~al.}}{{Yun, Jeong, Kim, Kang, and Kim}}}
\bibcite{zhang2019root}{{44}{2019}{{Zhang \& Sennrich}}{{Zhang and Sennrich}}}
\bibcite{zhang2022opt}{{45}{2022}{{Zhang et~al.}}{{Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.}}}
\bibcite{zhu2023minigpt}{{46}{2023}{{Zhu et~al.}}{{Zhu, Chen, Shen, Li, and Elhoseiny}}}
\bibstyle{arxiv}
\citation{hendrycks2016gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {A}Gradient of Pattention Layer}{14}{appendix.A}\protected@file@percent }
\newlabel{appedix:grad}{{A}{14}{Gradient of Pattention Layer}{appendix.A}{}}
\newlabel{eq:derivation_softmax}{{16}{14}{Gradient of Pattention Layer}{equation.A.16}{}}
\newlabel{eq:derivation_gelul2}{{26}{14}{Gradient of Pattention Layer}{equation.A.26}{}}
\citation{Gokaslan2019OpenWeb}
\@writefile{toc}{\contentsline {section}{\numberline {B}Zero initialization in Tokenformer\xspace  }{15}{appendix.B}\protected@file@percent }
\newlabel{appedix:zero}{{B}{15}{Zero initialization in \ourmethod }{appendix.B}{}}
\newlabel{eq:token-parameter-1}{{27}{15}{Zero initialization in \ourmethod }{equation.B.27}{}}
\newlabel{eq:token-parameter-2}{{28}{15}{Zero initialization in \ourmethod }{equation.B.28}{}}
\newlabel{eq:token-parameter-2}{{29}{15}{Zero initialization in \ourmethod }{equation.B.29}{}}
\newlabel{eq:token-parameter-1}{{30}{15}{Zero initialization in \ourmethod }{equation.B.30}{}}
\newlabel{eq:token-parameter-2}{{31}{15}{Zero initialization in \ourmethod }{equation.B.31}{}}
\newlabel{eq:token-parameter-2}{{32}{15}{Zero initialization in \ourmethod }{equation.B.32}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}Tabular Main Results}{15}{appendix.C}\protected@file@percent }
\citation{black2022gpt}
\citation{loshchilov2017decoupled}
\citation{mahoney2011large}
\citation{Gokaslan2019OpenWeb}
\citation{chen2015net2net}
\citation{loshchilov2017decoupled}
\citation{peng2023rwkv,mamba}
\citation{gao2020pile}
\citation{biderman2023pythia}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces  Tabular results of Figure \ref  {fig:scaling_individual}. The perplexity of models trained with different numbers of schemes is compared. Transformers are trained from scratch, while {Tokenformer\xspace  } are progressively scaled up via parameter resuing. When trained with the same number of tokens (30B), {Tokenformer\xspace  } demonstrates superior performance. }}{16}{table.6}\protected@file@percent }
\newlabel{tab:main2}{{6}{16}{Tabular results of Figure \ref {fig:scaling_individual}. The perplexity of models trained with different numbers of schemes is compared. Transformers are trained from scratch, while {\ourmethod } are progressively scaled up via parameter resuing. When trained with the same number of tokens (30B), {\ourmethod } demonstrates superior performance}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces  Tabular results of Figure \ref  {fig:scaling_accum}. Due to parameter reusing, {Tokenformer\xspace  } achieves the same performance while using a much lower number of training tokens when scaling up model sizes. }}{16}{table.7}\protected@file@percent }
\newlabel{tab:main1}{{7}{16}{Tabular results of Figure \ref {fig:scaling_accum}. Due to parameter reusing, {\ourmethod } achieves the same performance while using a much lower number of training tokens when scaling up model sizes}{table.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Experimental Details on Progressive model scaling}{16}{appendix.D}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {E}Experimental Details on Scaling without losing the well-learned distribution}{16}{appendix.E}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {F}Experiments on Language modeling Benchmarking}{16}{appendix.F}\protected@file@percent }
\newlabel{appedix:model_spec}{{F}{16}{Experiments on Language modeling Benchmarking}{appendix.F}{}}
\citation{fu2023hungry}
\citation{biderman2023pythia}
\citation{mamba}
\citation{fu2023hungry}
\citation{biderman2023pythia}
\citation{mamba}
\citation{biderman2023pythia}
\citation{mamba}
\citation{black2021gpt}
\citation{fu2023hungry}
\citation{zhang2022opt}
\citation{biderman2023pythia}
\citation{peng2023rwkv}
\citation{mamba}
\citation{black2021gpt}
\citation{fu2023hungry}
\citation{zhang2022opt}
\citation{biderman2023pythia}
\@writefile{lot}{\contentsline {table}{\numberline {8}{\ignorespaces  Details of Tokenformer model variants used in Section \ref  {sec:xp}. $\dag  $ indicates models whose key-value pairs are chosen to match the parameter numbers of Transformer of equivalent sizes. }}{17}{table.8}\protected@file@percent }
\newlabel{tab:model_spec}{{8}{17}{Details of Tokenformer model variants used in Section \ref {sec:xp}. $\dag $ indicates models whose key-value pairs are chosen to match the parameter numbers of Transformer of equivalent sizes}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {9}{\ignorespaces (\textbf  {Zero-shot Evaluations.}) The best results for each model size are highlighted in bold. We compare our models against open-source language models (LMs), including RNN-based methods, with various tokenizers, trained for up to 300B tokens. }}{17}{table.9}\protected@file@percent }
\newlabel{tab:llm_benchmark_all}{{9}{17}{(\textbf {Zero-shot Evaluations.}) The best results for each model size are highlighted in bold. We compare our models against open-source language models (LMs), including RNN-based methods, with various tokenizers, trained for up to 300B tokens}{table.9}{}}
