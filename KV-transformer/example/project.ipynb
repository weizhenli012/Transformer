{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer\n",
    "from transformer import Bert\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "pinyin_list = []\n",
    "hanzi_list = []\n",
    "vocab = set()\n",
    "max_length = 64\n",
    "\n",
    "with open(\"./zh.tsv\", errors='ignore', encoding='utf-8') as f:\n",
    "    contexts = f.readlines()\n",
    "    for line in contexts:\n",
    "        line = line .strip().split(\" \")\n",
    "        pinyin = line[1].split(\" \")\n",
    "        hanzi = line[2].split(\" \")\n",
    "        for p,h in zip(pinyin,hanzi):\n",
    "            vocab.add(p)\n",
    "            vocab.add(h)\n",
    "        pinyin = pinyin + [\"PAD\"]*(max_length-len(pinyin))\n",
    "        hanzi = hanzi + [\"PAD\"]*(max_length-len(hanzi))\n",
    "        if len(pinyin) <= max_length:\n",
    "            pinyin_list.append(pinyin)\n",
    "            hanzi_list.append(hanzi)\n",
    "\n",
    "vocab = [\"PAD\"] + list(sorted(vocab))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "pinyin_list = pinyin_list[:3000]\n",
    "hanzi_list = hanzi_list[:3000]\n",
    "\n",
    "def get_token_ids():\n",
    "    pinyin_ids = []\n",
    "    hanzi_ids = []\n",
    "    for pinyin,hanzi in zip(tqdm(pinyin_list,hanzi_list)):\n",
    "        pinyin_ids.append([vocab.index(p) for p in pinyin])\n",
    "        hanzi_ids.append([vocab.index(h) for h in hanzi])\n",
    "    return pinyin_ids,hanzi_ids\n",
    "\n",
    "class TextSampleDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, pinyin_ids, hanzi_ids):\n",
    "        super().__init__()\n",
    "        self.pinyin_ids = pinyin_ids\n",
    "        self.hanzi_ids = hanzi_ids\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pinyin_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.pinyin_ids[idx]), torch.tensor(self.hanzi_ids[idx])\n",
    "    \n",
    "from torch.utils.data import DataLoader\n",
    "loader = DataLoader(TextSampleDS(*get_token_ids()), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    def __init__(self, bert_encoder, d_model, vocab_size, dropout=0.1):\n",
    "        self.bert_encoder = bert_encoder\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        encoder = self.bert_encoder(input_ids, attention_mask=attention_mask)\n",
    "        encoder = self.dropout(encoder)\n",
    "        logits = self.output_layer(encoder)\n",
    "        return logits\n",
    "bert_encoder = Bert.BERT(vocab_size, d_model=768)\n",
    "model = BertModel(bert_encoder, 768, vocab_size, dropout=0.1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-8)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "            T_max = 2400, eta_min = 2e-6, last_epoch=-1)\n",
    "from accelerate import Accelerator\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, loader, lr_scheduler = accelerator.prepare(model, optimizer, loader, lr_scheduler)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    pbar = tqdm(loader)\n",
    "    for pinyin_ids, hanzi_ids in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        pinyin_ids = pinyin_ids.to(device)\n",
    "        hanzi_ids = hanzi_ids.to(device)\n",
    "        logits = model(pinyin_ids)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), hanzi_ids.view(-1))\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        pbar.set_description(f\"epoch:{epoch+1}, train_loss:{loss.item():.4f},\n",
    "                             lr:{lr_scheduler.get_last_lr()[0]:.6f}\")\n",
    "        \n",
    "torch.save(model.state_dict(), \"./model.pth\")\n",
    "#model.load_state_dict(torch.load(\"./model.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = torch.softmax(pred, dim=-1)\n",
    "    pred = torch.argmax(pred, dim=-1)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykan-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
